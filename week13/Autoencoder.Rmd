---
title: "Autoencoder"
subtitle: "빅데이터분석실무 13주차"
author: "ssongplay"
date: "`r format(Sys.Date())`"
output:
  html_document: 
    fig_height: 6
    fig_width: 10
    highlight: zenburn
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    fig_height: 6
    fig_width: 10
    toc: no
  word_document: 
    fig_height: 6
    fig_width: 9
---

```{r}
library(autoencoder)
df = read.csv("eeg_tr.txt")
names(df)
mat = as.matrix(df[,3:ncol(df)])
```

```{r}
par(mfrow = c(3,1))
for(i in 1:ncol(mat)){
  plot(mat[,i], type="o")
}
```

```{r}
## pca ##
pc = princomp(mat)
biplot(pc)
sc = pc$scores[,4:18]
recon_error = rowSums(sqrt(sc^2))  # reconstruction error dk
plot(recon_error)
pred = predict(pc,mat)
plot(sqrt(rowSums(pred[,4:18]^2)))
```

```{r}
## autoencoder
nl=3 ## number of layers (default is 3: input, hidden, output)
unit.type = "logistic" ## specify the network unit type, i.e., the unit's
## activation function ("logistic" or "tanh")
N.input = 15 ## number of units (neurons) in the input layer (one unit per pixel)
N.hidden = 5 ## number of units in the hidden layer
lambda = 0.0002 ## weight decay parameter
beta = 6 ## weight of sparsity penalty term
rho = 0.01 ## desired sparsity parameter
epsilon <- 0.001 ## a small parameter for initialization of weights
## as small gaussian random numbers sampled from N(0,epsilon^2)
max.iterations = 100 ## number of iterations in optimizer
fit <- autoencode(X.train=mat,nl=nl,N.hidden=N.hidden,
unit.type=unit.type,lambda=lambda,beta=beta,rho=rho,epsilon=epsilon,
optim.method="BFGS",max.iterations=max.iterations,
rescale.flag=TRUE,rescaling.offset=0.001)
attributes(fit)
pred= predict(fit,mat)
recon = pred$X.output
dim(recon)
recon_error = rowSums(mat - recon)
plot(recon_error)
```
