---
title: "Linear Regression"
subtitle: "빅데이터분석실무 7-8주차"
author: "ssongplay"
date: "`r format(Sys.Date())`"
output:
  html_document: 
    fig_height: 6
    fig_width: 10
    highlight: zenburn
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    fig_height: 6
    fig_width: 10
    toc: no
  word_document: 
    fig_height: 6
    fig_width: 9
---

# 뉴튼-랩슨 구현하기 

```{r}
newton_raphson = function(f, fp, init, tol = 1e-9, max = 100) {
  # tol은 0에 가까운 값이나 0은 아님
  iter = 0 # iteration 시작,
  oldx = init # x0를 의미
  x = oldx + 10*tol  #밑에 x-oldx가 tol보다 커야되는 조건
  
  #converge
  while(abs(x-oldx)>tol){  # 수렴할 때까지
    iter = iter + 1
    if(iter>max){
      stop("there is no solution")  # 100번 돌 때까지 수렴하지 않으면 출력
    }
    oldx = x
    x = x - f(x)/fp(x)   #뉴튼랩슨 구현
  }
  return (paste("solution is", x))
}
```


# 할선법

```{r}
secant = function(f, init, tol = 1e-9, max = 100){
  i = 0
  oldx = init
  oldfx = f(init)
  x = oldx + 10*tol
  
  #convergence
  while(abs(x-oldx)>tol){
    i = i + 1
    if(i>max) stop("there is no solution")
    fx = f(x)
    newx = x - f(x)*((x-oldx)/(fx-oldfx))  #secant, 할선의 방정식 (즉, y=0을 지나는 x 찾기)
    oldx = x   
    x = newx
    cat("iteration", i, "value of x is:", x, "\n")
  }
  return(x)
}
```


- $$ fx = e^{-x} - x = 0 $$ 의 근을 뉴튼랩슨, 할선법으로 구해보자.

```{r}
f = function(x) exp(-x)-x
fp = function(x) -(exp(-x) + 1)
print("newton_raphson")
newton_raphson(f,fp, init=0)
print("secant")
secant(f, init=0)

```


# Gradient Descent
```{r}
g_desc = function(fp, x, h=1e-2, tol=1e-4, m=1e3){
  iter = 0
  oldx = x
  x = x - h*fp(x)  # initial gradient 만큼 이동
  while(abs(x-oldx)>tol){
    iter = iter + 1 
    if(iter>m) stop ("max iteration")
    oldx = x
    x = x - h*fp(x)   # 부호가 + 라면 gradient ascent
  }
  return(x)
}
```

```{r}
# 0.25*x^4+x^3-x-1 
f <- readline(prompt = "Function? ")  # 수식 입력
f = parse(text = f)   # 수식을 expression으로 
fd = D(f, "x")  # text 형태로 된 수식을 미분
fd
```

```{r}
fd = function(x) 0.25 * (4 * x^3) + 3 * x^2 - 1 

Res = g_desc(fd, -1)
Res

Res1 = g_desc(fd, -10)
Res1
```

```{r}
f = function(x) 0.25*x^4+x^3-x-1
curve(f, -4, 2)
abline(v=c(-1), lty=2, col="blue")
abline(v=c(Res), lty=2, col="red")
# 파란색 : 시작점
# 빨간색 : 수렴점점
```

```{r}
f = function(x) 0.25*x^4+x^3-x-1
curve(f, -4, 2)
Res = g_desc(fd, 2)
abline(v=c(-1), lty=2, col="blue")
abline(v=c(Res), lty=2, col="red")
# 파란색 : 시작점
# 빨간색 : 수렴점점
```


# 단일변수의 최적화 vs 다중변수의 최적화
```{r}
# 3차원 플롯
x <- seq(-5, 5, length = 100)
y <- x
f <- function(x,y) {(x-1)^2 + (2*y-1)^2 }
z <- outer(x, y, f)
op <- par(bg = "white")
persp(x, y, z, theta=45, phi=30, col="lightblue")
```


```{r}
# 변수 2개일때 gradient descent
g_bi = function(fp, x, h=1e-2, tol=1e-4, m=1e3){
  iter = 0
  oldx = x
  x = x - h*fp(x)  #initial gradient만큼 이동
  b = x - oldx
  vecnorm = sqrt(sum(b^2))   # 벡터놈 정의
  
  while((vecnorm)>tol){    # 다변수에서는 이동량을 벡터놈으로 정의 
    iter = iter + 1
    if(iter > m) return(x)  # 수렴횟수보다 넘어가면 결과값 산출
    oldx = x
    x = x - h*fp(x)  # 부호가 +라면 gradient ascent
  }
  return(x)
}
```

```{r}
fp = function(x){  # 다변수미분을 위한 방정식 셋팅 (다변수 도함수)
  x1 = 2*x[1] - 2  # x[1] : x벡터 중 첫번째 element
  x2 = 8*x[2] - 4  
  return(c(x1,x2))
}
```

```{r}
g_bi(fp, c(0,0), m=1000)  # 수렴 횟수를 1000으로 제한, 결과는?
```


# Gradient & Jacobian & Hessian
```{r}
library(numDeriv)
func <- function(x) {
  a <- x[1]
  b <- x[2]
  rez <- (a^2)*(b^3)
  rez
}
grad(func, c(1,2))
jacobian(func, c(1,2))
hessian(func, c(1,2))
```

```{r}
f <- function(x) {c(x[1]^2 + x[2]^2 - 1, sin(pi*x[1]/2) + x[2]^3)}

g <- function(x) {(x[1])^3+(x[2])^2}

jacobian(f, c(1,1)) 

hessian(g, c(1,1))

```

# Genetic Algorithm
- 1975년 미시간대학의 John Holland가 제안한 방법
- 메타 휴리스틱스 중, 가장 일반적이고 높은 활용도를 보임
- Gene : 변수 하나의 값(value)
- Chromosome : 변수의 조합, 하나의 개체를 의미
- Fitness function : 최적화의 대상, 즉, 목적함수를 나타냄
- Population : 개체들의 조합, 가능한 가설들(혹은 해공간)의 집합
- Evaluation : 각 해공간에 대해 fitness 값을 산출하는 행위
- 유전 연산자 : 우성인자 선택 (elitism selection)/교차(cross over)/돌연변이 (mutation)
- 유전자의 형태를 표현하고 더 나은해를 탐색하는 형태의 방법을 통칭 GP(genetic programming) 이라 하며 다양한 방법론 존재 (트리/스택/선형/그래프 기반)


```{r}
library(GA) # GA 라이브러리 가져오기
rastrigin = function(x1,x2){20+x1^2 + x2^2 -10*(cos(2*pi*x1)+cos(2*pi*x2))} # rastrigin 함수
x1 = seq(-5,5,by = 0.1)
x2 = seq(-5,5,by = 0.1) ## x,y의 영역

f = outer(x1,x2, rastrigin) # 함수의 외적

library(plot3D)
persp3D(x1,x2,f, theta = 50, phi=20) # x,y에 따른 외적값

ga = ga(type = "real-valued", fitness  = function(x) - rastrigin(x[1],x[2])
        ,lower = c(-5,-5), upper = c(5,5), popSize = 1000,maxiter = 100, pcrossover = 0.8, 
        pmutation = 0.1) #벡터

summary(ga)

plot(ga) # GA 함수 수렴 시각화
ga@solution # 최소값을 만족시키는 x1, x2

rastrigin(ga@solution[1],ga@solution[2]) # 최소목적함수값

```

